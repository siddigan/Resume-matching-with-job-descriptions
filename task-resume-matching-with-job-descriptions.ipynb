{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/siddigantm/task-resume-matching-with-job-descriptions?scriptVersionId=143623087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-20T08:12:55.413411Z","iopub.execute_input":"2023-09-20T08:12:55.413988Z","iopub.status.idle":"2023-09-20T08:12:56.393912Z","shell.execute_reply.started":"2023-09-20T08:12:55.41395Z","shell.execute_reply":"2023-09-20T08:12:56.393002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:12:56.395926Z","iopub.execute_input":"2023-09-20T08:12:56.396979Z","iopub.status.idle":"2023-09-20T08:13:11.896565Z","shell.execute_reply.started":"2023-09-20T08:12:56.396943Z","shell.execute_reply":"2023-09-20T08:13:11.895146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:13:11.898882Z","iopub.execute_input":"2023-09-20T08:13:11.899325Z","iopub.status.idle":"2023-09-20T08:13:25.26902Z","shell.execute_reply.started":"2023-09-20T08:13:11.899287Z","shell.execute_reply":"2023-09-20T08:13:25.26789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pypdf import PdfReader\nfrom nltk import pos_tag, sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef extract_text_from_pdf(file_path):\n    reader = PdfReader(file_path)\n    text = \"\".join(page.extract_text() for page in reader.pages)\n    return text\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    sentences = sent_tokenize(text)\n    features = {'feature': \"\"}\n    stop_words = set(stopwords.words(\"english\"))\n    for sent in sentences:\n        if any(criteria in sent for criteria in ['skills', 'education']):\n            words = word_tokenize(sent)\n            words = [word for word in words if word not in stop_words]\n            tagged_words = pos_tag(words)\n            filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n            features['feature'] += \" \".join(filtered_words)\n    return features\n\ndef process_resume_data(df):\n    id = df['ID']\n    category = df['Category']\n    text = extract_text_from_pdf(f\"/kaggle/input/resume-dataset/data/data/{category}/{id}.pdf\")\n    features = preprocess_text(text)\n    df['Feature'] = features['feature']\n    return df\n\ndef get_embeddings(text, model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n    return embeddings\n\ndef print_top_matching_resumes(result_group):\n    for i in range(15):\n        print(\"\\nJob ID:\", i)\n        print(\"Cosine Similarity | Domain Resume | Domain Description\")\n        print(result_group.get_group(i)[['similarity', 'domainResume', 'domainDesc']])\n\ndef main():\n    resume_data = pd.read_csv(\"/kaggle/input/resume-dataset/Resume/Resume.csv\")\n    resume_data = resume_data.drop([\"Resume_html\"], axis=1)\n    resume_data = resume_data.apply(process_resume_data, axis=1)\n    resume_data = resume_data.drop(columns=['Resume_str'])\n    resume_data.to_csv(\"/kaggle/working/resume_data.csv\", index=False)\n\n    job_description = pd.read_csv(\"/kaggle/input/resume-and-job-description/training_data.csv\")\n    job_description = job_description[[\"job_description\", \"position_title\"]][:15]\n    job_description['Features'] = job_description['job_description'].apply(lambda x : preprocess_text(x)['feature'])\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model_name = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    model.to(device)\n\n    job_desc_embeddings = np.array([get_embeddings(desc, model_name) for desc in job_description['Features']]).squeeze()\n    resume_embeddings = np.array([get_embeddings(text, model_name) for text in resume_data['Feature']]).squeeze()\n\n    result_df = pd.DataFrame(columns=['jobId', 'resumeId', 'similarity', 'domainResume', 'domainDesc'])\n\n    for i, job_desc_emb in enumerate(job_desc_embeddings):\n        similarities = cosine_similarity([job_desc_emb], resume_embeddings)\n        top_k_indices = np.argsort(similarities[0])[::-1][:5]\n        for j in top_k_indices:\n            result_df.loc[i+j] = [i, resume_data['ID'].iloc[j], similarities[0][j], resume_data['Category'].iloc[j], job_description['position_title'].iloc[i]]\n\n    result_df = result_df.sort_values(by='similarity', ascending=False)\n    result_group = result_df.groupby(\"jobId\")\n    print_top_matching_resumes(result_group)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:40:03.314282Z","iopub.execute_input":"2023-09-20T08:40:03.31486Z","iopub.status.idle":"2023-09-20T09:06:07.595296Z","shell.execute_reply.started":"2023-09-20T08:40:03.314806Z","shell.execute_reply":"2023-09-20T09:06:07.594124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}